\section{Alano Algorithm for a large-scale Network}
%\section{Large-Scale Networks}
\label{PCN}

%Expectation			#meester1996continuum
%Geometric distribution	#Motwani1995Randomized
%Temperature 			#flammini2007real

%background and instances
In a large-scale network, nodes are not fully-connected and communications may fail due to concurrent transmissions. 
When we do not consider the energy constraints of nodes, we propose a nearly optimal algorithm for a large-scale network, which implies $\theta_i = 1$ for all node $u_i$.
Suppose the locations of nodes obey some distribution, we propose Alano algorithm and analyze its performance for two common used distribution (uniform distribution and Gaussian distribution).

\subsection{Alano Algorithm}
Suppose the locations of nodes obey some distribution and each node $u_i$ is aware of its position coordinates $(x_i, y_i)$.
Then $u_i$ could compute its local density by the following general function: % \cite{meester1996continuum, wang2015connectivity}.
$$f(x,y)=
\begin{cases}
\varphi(x,y)& (x,y)\in D\\
0& (x,y)\notin D
\end{cases}$$
where $(x,y)$ is a position coordinate, and $D$ is the network covering area.
%For each node $u_i$ with position $(x_i,y_i)$, T
Denote the range of $u_i$'s neighbors' positions as $R_i$,  and any neighbor with coordinates $(x,y) \in R_i$ suits:
$$
(x-x_i)^2+(y-y_i)^2 \leq \Delta^2
$$
where $\Delta$ is the communication range. Then, node $u_i$'s expected number of neighbors (denote as $\widetilde{n_i}$) is:
$$
\widetilde{n_i} = N\iint_{R_i} f(x,y)\,dx\,dy - 1.
$$

In a large-scale network, we ignore the boundary area of the network.  %and assume nodes in the network are of an enormous quantity. 
Note that, when the network covering area $D$ is much larger than the area $R_i$ of node $u_i$, we have:
\begin{equation}
\label{eqnNB}
\widetilde{n_i} \simeq N\pi \Delta^2 \varphi(x_i,y_i).
\end{equation}


%Alano
We present \textbf{Alano}, a randomized algorithm for node $u_i$ in Alg. \ref{Alano}.
By computing the expected number of nodes, $u_i$ turn to be in transmitting state or listening state according to the generated probability in Line \ref{pro}.


\begin{algorithm}
\caption{Alano Algorithm}
\label{Alano}
\begin{algorithmic}[1]
%\STATE $\widetilde{n_i} = N\iint_{R_i} \varphi(x,y)\,dx\,dy$;
\STATE Set tranmist probability $p_t^i := \frac{1}{\widetilde{n_i}+1}$, $t := 0$;  \label{pro}
\WHILE {$t\leq T$}		\label{time}%Threshold 
	\STATE Generate a random number $\epsilon \in (0,1)$;  
   	 \IF{$\epsilon < p_t$}
    		\STATE Transmit a message containing $u_i$'s information including its ID through the channel;
	\ELSE
    		\STATE Listen on the channel. If receive a message successfully, decode the message and record the sender's ID;
	\ENDIF
	\STATE $t:= t+1$;
\ENDWHILE
\end{algorithmic}
\end{algorithm}

In the following parts, we first consider a general situation that nodes in the network are uniform distributed. We derive a proof that the probability chosen in Alano is the optimal one and show the discovery latency will not be much larger than its expectation. $T$ in Alg. \ref{Alano} Line \ref{time} is the time threshold and can be set as the latency bound. Then we analyze a more common situation that nodes obey Gaussian Distribution, we present an approximation analysis that the discovery latency is not be much larger than that of uniform distribution.


\subsection{Analysis for Uniform Distribution}
\label{uniform}
%background
Uniform distribution is a basic one for deployment of wireless networks.
For instance, to monitor an unknown area, many sensors are deployed uniformly to collect information, such as temperature and humidity \cite{flammini2007real}. The nodes are evenly deployed and the density function is:
$$f(x)=
\begin{cases}
\frac{1}{A}& (x,y)\in D\\
0& (x,y)\notin D
\end{cases}$$
where $A$ is the area of $D$.
By Eqn. (\ref{eqnNB}), $u_i$'s expected number of neighbors is $\widetilde{n_i} = \frac{N\pi \Delta^2}{A}$ and the probability in Line 2 is set as $p_t^i = \frac{1}{\widetilde{n_i}+1}=\frac{A}{N\pi \Delta^2+A}$. 
\begin{lemma}
Alg. \ref{Alano} achieves optimal discovery latency for uniform distribution by setting $p_t^i = \frac{1}{\widetilde{n_i}}$.
\end{lemma}
\begin{IEEEproof}
For any two nodes $u_i$ abd $u_j$ in the uniform distribution, we get $\widetilde{n_i} = \widetilde{n_j} = \widetilde{n}$ and $p_t^j = p_t^ i = p_t = \frac{1}{\widetilde{n}+1}$.



From Alg. \ref{Alano}, the probability that node $u_i$ discovers a specific neighbor (such as $u_j$) successfully in a time slot (denote as $p_s$) is:
$$
p_s = p_t{(1-p_t)}^{\widetilde{n}-1}(1-p_t).
$$
In order to compute the maximum probability to discover a node, we compute the differential function of $p_s$ as:
$$
\frac{d(p_s)}{d(p_t)} = {(1-p_t)}^{\widetilde{n}}-\widetilde{n}p_t{(1-p_t)}^{\widetilde{n}-1}.
$$
It is easy to verify that when $p_t=\frac{1}{\widetilde{n}+1}$, $p_s$ gets the maximum value:
$$p_s = \frac{1}{\widetilde{n}+1}{(1-\frac{1}{\widetilde{n}+1})}^{\widetilde{n}} \approx \frac{1}{(\widetilde{n}+1)e}.$$

Therefore, the probability chosen in Alano algorithm for transmitting is optimal.
\end{IEEEproof}



\begin{theorem}
Alg. \ref{Alano} discovers all neighbors for node $u_i$ within $T=\Theta(n\ln n)$ time slots with high probability.
\end{theorem}
\begin{IEEEproof} First, we show that the expected discovery latency of a node $u_i$ is bounded by $O(n\ln n)$.

Let $W$ be a random variable that denotes the time a node spends discovering all neighbors. Denote $W_j$ as a random variable representing the cost number of the time slots to discover a new neighbor after $j-1$ neighbors have been discovered. It is easy to check that $W_j$ follows Geometric distribution with parameter $p(j)$ where $p(j)=(\widetilde{n}-j+1)p_s$ \cite{Motwani1995Randomized}. The expectation
of $W_j$ can be computed as:
$$
E[W_j]=\frac{1}{p(j)}=\frac{1}{(\widetilde{n}-j+1)p_s}.
$$

Then, the expectation discovery latency of node $u_i$ is:
$$
E[W] = \sum_{j=1}^{\widetilde{n}}E[W_j] \approx (\widetilde{n}+1)e(\ln (\widetilde{n}+1) + \Theta(1)) = \Theta(n\ln n).
$$
Thus the expected discovery latency is bounded by $O(n\ln n)$.

Then, we show that node $u_i$ can discovery all neighbors bounded by $O(n\ln n)$ time slots.

Since $W_j$ follows Geometric distribution, $Var[W_j]=\frac{1-p_j}{p_j^2}$, and the variance of discovery latency of $W$ is computed as:
\begin{displaymath}
\begin{split}
 Var[W] %& =Var[\sum_{j=1}^{p_nN}W_j]=\sum_{j=1}^{p_nN}Var[W_j]+\sum_{j\ne k}Cov[W_j,W_k] \\
 =\sum_{j=1}^{n}Var[W_j]
%=\frac{1}{p_{suc}^2}\sum_{j=1}^{p_nN}\frac{1}{j^2}-\frac{1}{p_{suc}}\sum_{j=1}^{p_nN}\frac{1}{j} \\
 \le\frac{\pi^2}{6p_{s}^2}-\frac{H_n}{p_{s}}.
\end{split}
\end{displaymath}

By \emph{Chebyshev's inequality}, the probability that the discovery latency is $2$ times larger than the expectation is
\begin{displaymath}
\begin{split}
P[W\ge2E[W]]%=P[|W-E[W]|\ge E[W]]
\le\frac{Var[W]}{{E[W]}^2}
%&\le\frac{\frac{\pi^2}{6p_{suc}^2}-\frac{H_{p_nN}}{p_{suc}}}{\frac{H_{p_nN}^2}{p_{suc}^2}}=
\le\frac{\pi^2}{6H_{n}^2}-\frac{p_{s}}{H_n}.
\end{split}
\end{displaymath}
where $H_n$ is the $n$-th Harmonic number, i.e., $H_n = lnn + \Theta(1)$. For a large $n$, $P[W\ge2E[W]]$ is close to $0$. That is, the time for a node to discover all neighbors is very likely to be smaller than $2$ times of the expectation. Therefore, $W$ is also bounded by $O(n\ln n)$ with high probability.
\end{IEEEproof}



\subsection{Analysis of Gaussian Distribution}
\label{normal}
%background
Gaussian distribution is common adopted in wireless network. For instance, an intrusion detection application needs larger detection
probability around important entities\cite{wang2013gaussian}. We assume the positions of nodes obey 2D Gaussian distribution, and we present a theoretical proof that the discovery latency is not much larger than that of uniform distribution. Without loss of generality, we consider $(x,y) \sim N(0,1,0,1,0)$.

\begin{theorem}
Alg. \ref{Alano} discovers all neighbors for node $u_i$ within $T=\Theta(n\ln n)$ time slots with high probability.
\end{theorem}

\begin{IEEEproof}
Denote the approximate neighbors of node $u_i$ as set $S_i = \{u_j | d(u_i, u_j) \leq \Delta \}$. When nodes obey Gaussian distribution, the probability that node $u_i$ discovers a certain neighbor node $u_{j}$ successfully in a time slot (denote as $p_{s}$) can be formulated as:
$$
p_{s}^i = (1-p_t^i) \cdot p_t^{j} \cdot \prod_{u_j \in S_i, u_j \neq u_k}(1-p_t^{k}).
$$
Denote $p_t^{max} = \max_{u_j \in S_i}\{p_t^{j}\}, p_t^{min} = \min_{u_j \in S_i}\{p_t^{j}\}$,
for every $u_j \in S_i$, we have:
\begin{equation*}
\begin{split}
(1-p_t^i)p_t^{min}{(1-p_t^{max})}^{\widetilde{n_i}-1} \leq p_{s}^i  \\ %\leq (1-p_t^i)p_t^{max}{(1-p_t^{min})}^{\widetilde{n_i}-1}
p_{s}^i \leq (1-p_t^i)p_t^{max}{(1-p_t^{min})}^{\widetilde{n_i}-1}
\end{split}
\end{equation*}

Denote: 
\begin{align*}
&P = (1-p_t^i)p_t^{min}{(1-p_t^{max})}^{\widetilde{n_i}-1}. \\
&Q = (1-p_t^i)p_t^{max}{(1-p_t^{min})}^{\widetilde{n_i}-1}.
\end{align*}

We derive the expectation of $W_j$ for $1 \leq j \leq n_i$ as:
$$
\frac{1}{(\widetilde{n_i}-j+1)Q} \quad \leq \quad E[W_j] \quad \leq \quad \frac{1}{(\widetilde{n_i}-j+1)P}.
$$

Combine the equations to derive:
$$
\frac{1}{Q}H_n  \quad \leq \quad E[\sum_{j=1}^{\widetilde{n_i}}W_j]  \quad \leq \quad \frac{1}{P}H_n.
$$

Since:
$$
p_t^{min} = \frac{1}{\widetilde{n}_{max}+1}, \quad p_t^{max} = \frac{1}{\widetilde{n}_{min}+1}.
$$
where $\widetilde{n}_{max} = \max\{\widetilde{n}_j | u_j\in S_i \}$, $\widetilde{n}_{min} = \min\{\widetilde{n}_j | u_j\in S_i \}$. 
Thus:
\begin{align*}
&\widetilde{n_i} \simeq N\pi \Delta^2 \frac{1}{2\pi}\mathrm{e}^{-\frac{x_i^2+y_i^2}{2}}.							\\
&\widetilde{n}_{max}  \simeq N\pi \Delta^2 \frac{1}{2\pi}\mathrm{e}^{-\frac{{(\sqrt{x_i^2+y_i^2}-\Delta^2)}^2}{2}}  = \alpha\widetilde{n_i}.\\
&\widetilde{n}_{min}  \simeq N\pi \Delta^2 \frac{1}{2\pi}\mathrm{e}^{-\frac{{(\sqrt{x_i^2+y_i^2}+\Delta^2)}^2}{2}}  = \beta\widetilde{n_i}.
\end{align*}
and we get:
$$
\frac{1}{Q}H_n \simeq \beta\mathrm{e}^{\frac{1}{\alpha}}n\ln n, \quad \frac{1}{P}H_n \simeq \alpha\mathrm{e}^{\frac{1}{\beta}}n\ln n. 
$$

Thus $E[W]$ can be bounded within $\Theta(n\ln n)$ time slots with high probability.
Similarly the bounded latency can be proved to be $W=O(nlnn)$ by \emph{Chebyshev's inequality} in the same way as
uniform distribution.
\end{IEEEproof}


